{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#  a fully-connected ReLU network\n",
    "only have one hidden layer using pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## project dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Code in file tensor/two_layer_net_tensor.py\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T11:04:11.206402Z",
     "start_time": "2023-06-15T11:04:11.187812900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T09:18:11.370489Z",
     "start_time": "2023-06-15T09:18:11.322738Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## set up parameter and hyperparameter\n",
    "### torch api\n",
    "1. `torch.randn`\n",
    "2. `h_relu.mm(w2) matrix *`\n",
    "3. `h.clamp(min=0)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T09:19:30.193392700Z",
     "start_time": "2023-06-15T09:19:30.170416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-15T09:43:06.333822700Z",
     "start_time": "2023-06-15T09:43:02.183753400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.871203608061478e-06\n",
      "1 1.8785796100928565e-06\n",
      "2 1.8772682324197376e-06\n",
      "3 1.8719372292252956e-06\n",
      "4 1.8654804989637341e-06\n",
      "5 1.8617549812915968e-06\n",
      "6 1.867563696578145e-06\n",
      "7 1.8672508304007351e-06\n",
      "8 1.8645096133695915e-06\n",
      "9 1.8575565263745375e-06\n",
      "10 1.8467956124368357e-06\n",
      "11 1.8485744703866658e-06\n",
      "12 1.8425350845063804e-06\n",
      "13 1.8384705526841572e-06\n",
      "14 1.8395709275864647e-06\n",
      "15 1.8307841855857987e-06\n",
      "16 1.8296416328666965e-06\n",
      "17 1.8339870848649298e-06\n",
      "18 1.8323153199162334e-06\n",
      "19 1.8313990040041972e-06\n",
      "20 1.847232852014713e-06\n",
      "21 1.8512205315346364e-06\n",
      "22 1.8411442397336941e-06\n",
      "23 1.827448386393371e-06\n",
      "24 1.806582986318972e-06\n",
      "25 1.796234982975875e-06\n",
      "26 1.8024869632426999e-06\n",
      "27 1.797962454475055e-06\n",
      "28 1.795892785594333e-06\n",
      "29 1.7877433720059344e-06\n",
      "30 1.7825217355493805e-06\n",
      "31 1.7944264527613996e-06\n",
      "32 1.7984000351134455e-06\n",
      "33 1.8046764580503805e-06\n",
      "34 1.79800611022074e-06\n",
      "35 1.796085143723758e-06\n",
      "36 1.784595269782585e-06\n",
      "37 1.7817997104430106e-06\n",
      "38 1.7780847656467813e-06\n",
      "39 1.7820391349232523e-06\n",
      "40 1.7802871070671245e-06\n",
      "41 1.7656311683822423e-06\n",
      "42 1.7729423689161194e-06\n",
      "43 1.7756647139322013e-06\n",
      "44 1.7761816479833215e-06\n",
      "45 1.7726522401062539e-06\n",
      "46 1.7840982309280662e-06\n",
      "47 1.7912611838255543e-06\n",
      "48 1.7845545698946808e-06\n",
      "49 1.79494736585184e-06\n",
      "50 1.797861614249996e-06\n",
      "51 1.7991068261835608e-06\n",
      "52 1.795673483684368e-06\n",
      "53 1.7763270534487674e-06\n",
      "54 1.7717694618113455e-06\n",
      "55 1.7609633005122305e-06\n",
      "56 1.7625204691285035e-06\n",
      "57 1.7582137843419332e-06\n",
      "58 1.74918989159778e-06\n",
      "59 1.7459257151131169e-06\n",
      "60 1.744608653098112e-06\n",
      "61 1.7448958260501968e-06\n",
      "62 1.7495755173513317e-06\n",
      "63 1.7456711702834582e-06\n",
      "64 1.7395745999237988e-06\n",
      "65 1.7316930325250723e-06\n",
      "66 1.714842937872163e-06\n",
      "67 1.7249832353627426e-06\n",
      "68 1.7313070657110075e-06\n",
      "69 1.7226748241228051e-06\n",
      "70 1.727831090647669e-06\n",
      "71 1.7170274304589839e-06\n",
      "72 1.7104493963415734e-06\n",
      "73 1.7148386177723296e-06\n",
      "74 1.7117215520556783e-06\n",
      "75 1.7124696114478866e-06\n",
      "76 1.6925557702052174e-06\n",
      "77 1.6920905636652606e-06\n",
      "78 1.6906296878005378e-06\n",
      "79 1.6919119616432e-06\n",
      "80 1.6827719946377329e-06\n",
      "81 1.6886222056200495e-06\n",
      "82 1.6814494756545173e-06\n",
      "83 1.6759399841248523e-06\n",
      "84 1.6808094187581446e-06\n",
      "85 1.6721888869142276e-06\n",
      "86 1.6664459963067202e-06\n",
      "87 1.6721248812245904e-06\n",
      "88 1.657893790252274e-06\n",
      "89 1.6506510291947052e-06\n",
      "90 1.6523188151040813e-06\n",
      "91 1.6596242176092346e-06\n",
      "92 1.6658982531225774e-06\n",
      "93 1.6613578281976515e-06\n",
      "94 1.6571387959629647e-06\n",
      "95 1.6575675090280129e-06\n",
      "96 1.6576461803197162e-06\n",
      "97 1.6702181255823234e-06\n",
      "98 1.6651250689392327e-06\n",
      "99 1.6610424609098118e-06\n",
      "100 1.6605765722488286e-06\n",
      "101 1.6560048834435293e-06\n",
      "102 1.6554862440898432e-06\n",
      "103 1.6431305311925826e-06\n",
      "104 1.6367233683922677e-06\n",
      "105 1.6394543536080164e-06\n",
      "106 1.6352357761206804e-06\n",
      "107 1.6357413414880284e-06\n",
      "108 1.6385841945520951e-06\n",
      "109 1.6371326410080655e-06\n",
      "110 1.6361378811780014e-06\n",
      "111 1.6341224409188726e-06\n",
      "112 1.6111999912027386e-06\n",
      "113 1.6126094806168112e-06\n",
      "114 1.6157002846739488e-06\n",
      "115 1.6159898450496257e-06\n",
      "116 1.6258843515970511e-06\n",
      "117 1.6110489013954066e-06\n",
      "118 1.613460653970833e-06\n",
      "119 1.6062432450780761e-06\n",
      "120 1.6059992731243256e-06\n",
      "121 1.6010452554837684e-06\n",
      "122 1.6157625850610202e-06\n",
      "123 1.6144944083862356e-06\n",
      "124 1.60983870500786e-06\n",
      "125 1.6184117157536093e-06\n",
      "126 1.6135816167661687e-06\n",
      "127 1.6029780454118736e-06\n",
      "128 1.596146034898993e-06\n",
      "129 1.5816084442121792e-06\n",
      "130 1.5735275837869267e-06\n",
      "131 1.5802567077116692e-06\n",
      "132 1.5789445342306863e-06\n",
      "133 1.5789937606314197e-06\n",
      "134 1.575489704919164e-06\n",
      "135 1.5694834019086557e-06\n",
      "136 1.5730840914329747e-06\n",
      "137 1.5751760429338901e-06\n",
      "138 1.5733313603050192e-06\n",
      "139 1.5819432519492693e-06\n",
      "140 1.5792453496032977e-06\n",
      "141 1.5675808526793844e-06\n",
      "142 1.562391730658419e-06\n",
      "143 1.557125187900965e-06\n",
      "144 1.5577910517095006e-06\n",
      "145 1.5583592585244332e-06\n",
      "146 1.5724590411991812e-06\n",
      "147 1.577433636157366e-06\n",
      "148 1.578457045070536e-06\n",
      "149 1.5765514262966462e-06\n",
      "150 1.5795749277458526e-06\n",
      "151 1.5689129213569686e-06\n",
      "152 1.5703965345892357e-06\n",
      "153 1.5662483292544493e-06\n",
      "154 1.571744860484614e-06\n",
      "155 1.5757998426124686e-06\n",
      "156 1.575271653564414e-06\n",
      "157 1.5793025340826716e-06\n",
      "158 1.5798996173543856e-06\n",
      "159 1.578868705109926e-06\n",
      "160 1.560856617288664e-06\n",
      "161 1.5613045434292872e-06\n",
      "162 1.5564805835310835e-06\n",
      "163 1.5547718703601277e-06\n",
      "164 1.5494268836846459e-06\n",
      "165 1.5549582030871534e-06\n",
      "166 1.545183749840362e-06\n",
      "167 1.5370183064078446e-06\n",
      "168 1.5437276488228235e-06\n",
      "169 1.539641971248784e-06\n",
      "170 1.5373907444882207e-06\n",
      "171 1.5431314750458114e-06\n",
      "172 1.5381746152343112e-06\n",
      "173 1.5501710777243716e-06\n",
      "174 1.545942495795316e-06\n",
      "175 1.537139837637369e-06\n",
      "176 1.5391999568237225e-06\n",
      "177 1.541372284918907e-06\n",
      "178 1.5405422573167016e-06\n",
      "179 1.5343257473432459e-06\n",
      "180 1.5306342220355873e-06\n",
      "181 1.5280461411748547e-06\n",
      "182 1.5304797216231236e-06\n",
      "183 1.5221962712530512e-06\n",
      "184 1.5119074987524073e-06\n",
      "185 1.510207880528469e-06\n",
      "186 1.5229354630719172e-06\n",
      "187 1.5321297723858152e-06\n",
      "188 1.5314918755393592e-06\n",
      "189 1.5175268117673113e-06\n",
      "190 1.5043142411741428e-06\n",
      "191 1.5136647562030703e-06\n",
      "192 1.5086880011949688e-06\n",
      "193 1.5115799669729313e-06\n",
      "194 1.5093776255525881e-06\n",
      "195 1.5026719211164163e-06\n",
      "196 1.501178644502943e-06\n",
      "197 1.4879688023938797e-06\n",
      "198 1.483133019064553e-06\n",
      "199 1.486596943323093e-06\n",
      "200 1.4903839655744378e-06\n",
      "201 1.4949711157896672e-06\n",
      "202 1.4934245200493024e-06\n",
      "203 1.4887422139509e-06\n",
      "204 1.4779235470996355e-06\n",
      "205 1.4662932699138764e-06\n",
      "206 1.4580937204300426e-06\n",
      "207 1.4492114814856905e-06\n",
      "208 1.4446800378209446e-06\n",
      "209 1.4375966657098616e-06\n",
      "210 1.4430936516873771e-06\n",
      "211 1.4358661246660631e-06\n",
      "212 1.4386109796760138e-06\n",
      "213 1.442058078282571e-06\n",
      "214 1.4392345519809169e-06\n",
      "215 1.4498559721687343e-06\n",
      "216 1.462926775275264e-06\n",
      "217 1.4666890137959854e-06\n",
      "218 1.464462229705532e-06\n",
      "219 1.459094505662506e-06\n",
      "220 1.447576551072416e-06\n",
      "221 1.4580605238734279e-06\n",
      "222 1.4640404515375849e-06\n",
      "223 1.4646923318650806e-06\n",
      "224 1.4646574300059e-06\n",
      "225 1.4684225106975646e-06\n",
      "226 1.4649149306933396e-06\n",
      "227 1.4535575019181124e-06\n",
      "228 1.4441941402765224e-06\n",
      "229 1.443322389604873e-06\n",
      "230 1.4392456932910136e-06\n",
      "231 1.4386957900569541e-06\n",
      "232 1.4334070783661446e-06\n",
      "233 1.4274805835157167e-06\n",
      "234 1.4294618040366913e-06\n",
      "235 1.4253453173296293e-06\n",
      "236 1.4251298807721469e-06\n",
      "237 1.4364179605763638e-06\n",
      "238 1.4326368500405806e-06\n",
      "239 1.4298454971140018e-06\n",
      "240 1.4329287978398497e-06\n",
      "241 1.4243778423406184e-06\n",
      "242 1.4202369129634462e-06\n",
      "243 1.4151348750601755e-06\n",
      "244 1.411981997989642e-06\n",
      "245 1.41497946515301e-06\n",
      "246 1.4239424217521446e-06\n",
      "247 1.4217336001820513e-06\n",
      "248 1.408647108291916e-06\n",
      "249 1.4086249393585604e-06\n",
      "250 1.4121637832431588e-06\n",
      "251 1.4058696251595393e-06\n",
      "252 1.4017444982528104e-06\n",
      "253 1.4140518942440394e-06\n",
      "254 1.4084835129324347e-06\n",
      "255 1.403278929501539e-06\n",
      "256 1.393562797602499e-06\n",
      "257 1.392740273331583e-06\n",
      "258 1.3909003655498964e-06\n",
      "259 1.3936681853010668e-06\n",
      "260 1.389480189573078e-06\n",
      "261 1.379986542815459e-06\n",
      "262 1.3870085240341723e-06\n",
      "263 1.3858821148460265e-06\n",
      "264 1.3856149507773807e-06\n",
      "265 1.3858890497431275e-06\n",
      "266 1.392269041389227e-06\n",
      "267 1.3845619832864031e-06\n",
      "268 1.3891819889977342e-06\n",
      "269 1.4011355915499735e-06\n",
      "270 1.3849244169250596e-06\n",
      "271 1.3765061339654494e-06\n",
      "272 1.3763226434093667e-06\n",
      "273 1.3826280564899207e-06\n",
      "274 1.3797846349916654e-06\n",
      "275 1.3732740171690239e-06\n",
      "276 1.378087517878157e-06\n",
      "277 1.370688096358208e-06\n",
      "278 1.3733407513427665e-06\n",
      "279 1.3739495443587657e-06\n",
      "280 1.371332473354414e-06\n",
      "281 1.3749175877819653e-06\n",
      "282 1.3786172985419398e-06\n",
      "283 1.3747285265708342e-06\n",
      "284 1.3709603763345513e-06\n",
      "285 1.3736530490859877e-06\n",
      "286 1.3801450222672429e-06\n",
      "287 1.3806691185891395e-06\n",
      "288 1.367730988022231e-06\n",
      "289 1.3686358215636574e-06\n",
      "290 1.3579044662037631e-06\n",
      "291 1.3554189308706555e-06\n",
      "292 1.3586803788712132e-06\n",
      "293 1.3531863487514784e-06\n",
      "294 1.3572827128882636e-06\n",
      "295 1.344140059700294e-06\n",
      "296 1.3432544392344425e-06\n",
      "297 1.340300855190435e-06\n",
      "298 1.3315731166585465e-06\n",
      "299 1.3265778306958964e-06\n",
      "300 1.3186834166845074e-06\n",
      "301 1.3187388958613155e-06\n",
      "302 1.3198130091041094e-06\n",
      "303 1.3194066923460923e-06\n",
      "304 1.307299953623442e-06\n",
      "305 1.3137461110090953e-06\n",
      "306 1.3137721452949336e-06\n",
      "307 1.3123192275088513e-06\n",
      "308 1.3053632983428543e-06\n",
      "309 1.292658680540626e-06\n",
      "310 1.2934459618918481e-06\n",
      "311 1.2908069493278163e-06\n",
      "312 1.2858139371019206e-06\n",
      "313 1.2885253681815811e-06\n",
      "314 1.2870608543380513e-06\n",
      "315 1.2941242175656953e-06\n",
      "316 1.2839283272114699e-06\n",
      "317 1.2842893966080737e-06\n",
      "318 1.2819350558856968e-06\n",
      "319 1.290796831199259e-06\n",
      "320 1.2941208069605636e-06\n",
      "321 1.2857665296905907e-06\n",
      "322 1.2788068488589488e-06\n",
      "323 1.276527200388955e-06\n",
      "324 1.2710440842056414e-06\n",
      "325 1.273429234061041e-06\n",
      "326 1.2761254311044468e-06\n",
      "327 1.2777882147929631e-06\n",
      "328 1.2723993449981208e-06\n",
      "329 1.2724583484668983e-06\n",
      "330 1.2674016716118786e-06\n",
      "331 1.2722821338684298e-06\n",
      "332 1.270491338800639e-06\n",
      "333 1.273480279451178e-06\n",
      "334 1.2760144727508305e-06\n",
      "335 1.2690295534412144e-06\n",
      "336 1.271036921934865e-06\n",
      "337 1.2694193856077618e-06\n",
      "338 1.2688800552496105e-06\n",
      "339 1.2685493402386783e-06\n",
      "340 1.2695325040112948e-06\n",
      "341 1.266017079615267e-06\n",
      "342 1.2623884231288685e-06\n",
      "343 1.2566260920721106e-06\n",
      "344 1.2608672932401532e-06\n",
      "345 1.253226173503208e-06\n",
      "346 1.2530446156233666e-06\n",
      "347 1.2546300922622322e-06\n",
      "348 1.2571532579386258e-06\n",
      "349 1.2580419479490956e-06\n",
      "350 1.2550051451398758e-06\n",
      "351 1.2523397572294925e-06\n",
      "352 1.2562858273668098e-06\n",
      "353 1.2473715287342202e-06\n",
      "354 1.249968931915646e-06\n",
      "355 1.2461872529456741e-06\n",
      "356 1.2375089681881946e-06\n",
      "357 1.2388677532726433e-06\n",
      "358 1.235460786119802e-06\n",
      "359 1.2318967037572293e-06\n",
      "360 1.2253044587851036e-06\n",
      "361 1.214003987115575e-06\n",
      "362 1.2154525848018238e-06\n",
      "363 1.207711648021359e-06\n",
      "364 1.2070581760781351e-06\n",
      "365 1.2108263263144181e-06\n",
      "366 1.2068184105373803e-06\n",
      "367 1.2015877928206464e-06\n",
      "368 1.2064825796187506e-06\n",
      "369 1.1986372783212573e-06\n",
      "370 1.1950265843552188e-06\n",
      "371 1.1922622888960177e-06\n",
      "372 1.1961743666688562e-06\n",
      "373 1.205162106998614e-06\n",
      "374 1.2076023949703085e-06\n",
      "375 1.2125086641390226e-06\n",
      "376 1.2166744909336558e-06\n",
      "377 1.2077532574039651e-06\n",
      "378 1.2134271401009755e-06\n",
      "379 1.2144349739173776e-06\n",
      "380 1.2266826843188028e-06\n",
      "381 1.2268434375073412e-06\n",
      "382 1.2232986819071812e-06\n",
      "383 1.2287113122511073e-06\n",
      "384 1.2219932159496238e-06\n",
      "385 1.219530986418249e-06\n",
      "386 1.2300384923946694e-06\n",
      "387 1.224723177983833e-06\n",
      "388 1.2174599532954744e-06\n",
      "389 1.2187869060653611e-06\n",
      "390 1.219144678543671e-06\n",
      "391 1.2281512908884906e-06\n",
      "392 1.2126193951189634e-06\n",
      "393 1.2135337783547584e-06\n",
      "394 1.2096331829525298e-06\n",
      "395 1.2008073326796875e-06\n",
      "396 1.2071849369021947e-06\n",
      "397 1.2041127774864435e-06\n",
      "398 1.1973271512033534e-06\n",
      "399 1.204679847432999e-06\n",
      "400 1.2011280432489002e-06\n",
      "401 1.2027427374050603e-06\n",
      "402 1.193063440041442e-06\n",
      "403 1.1973523896813276e-06\n",
      "404 1.1998009767921758e-06\n",
      "405 1.2086661627108697e-06\n",
      "406 1.206711090162571e-06\n",
      "407 1.2067652050973265e-06\n",
      "408 1.2072479194102925e-06\n",
      "409 1.2115115168853663e-06\n",
      "410 1.204533646159689e-06\n",
      "411 1.2006953511445317e-06\n",
      "412 1.199971165988245e-06\n",
      "413 1.2012335446343059e-06\n",
      "414 1.189404315482534e-06\n",
      "415 1.193289222101157e-06\n",
      "416 1.196300672745565e-06\n",
      "417 1.1951447049796116e-06\n",
      "418 1.2000534752587555e-06\n",
      "419 1.201155100716278e-06\n",
      "420 1.1965427120230743e-06\n",
      "421 1.2008528074147762e-06\n",
      "422 1.1984852790192235e-06\n",
      "423 1.1932703500860953e-06\n",
      "424 1.189378167509858e-06\n",
      "425 1.1868696674355306e-06\n",
      "426 1.1925017133762594e-06\n",
      "427 1.1900395975317224e-06\n",
      "428 1.1916595212824177e-06\n",
      "429 1.1908143733307952e-06\n",
      "430 1.1918896234419663e-06\n",
      "431 1.1925556009373395e-06\n",
      "432 1.192371200886555e-06\n",
      "433 1.197798610519385e-06\n",
      "434 1.1926780416615657e-06\n",
      "435 1.1917854862986133e-06\n",
      "436 1.1857783874802408e-06\n",
      "437 1.1830820767499972e-06\n",
      "438 1.1794434158218792e-06\n",
      "439 1.1778558928199345e-06\n",
      "440 1.172927341031027e-06\n",
      "441 1.1757537095036241e-06\n",
      "442 1.177816898234596e-06\n",
      "443 1.173865825876419e-06\n",
      "444 1.1752219961636001e-06\n",
      "445 1.164225523098139e-06\n",
      "446 1.1711907745848293e-06\n",
      "447 1.1677778957164264e-06\n",
      "448 1.1737515706045087e-06\n",
      "449 1.1780037993958103e-06\n",
      "450 1.1820667396023055e-06\n",
      "451 1.1671105539790005e-06\n",
      "452 1.1714679430951946e-06\n",
      "453 1.16544947559305e-06\n",
      "454 1.173358214145992e-06\n",
      "455 1.172271481664211e-06\n",
      "456 1.163759634437156e-06\n",
      "457 1.1681221394610475e-06\n",
      "458 1.1779234228015412e-06\n",
      "459 1.1724853266059654e-06\n",
      "460 1.1662782526400406e-06\n",
      "461 1.1679633189487504e-06\n",
      "462 1.174182330032636e-06\n",
      "463 1.1687624237310956e-06\n",
      "464 1.1735450016203686e-06\n",
      "465 1.166600668511819e-06\n",
      "466 1.1719766916939989e-06\n",
      "467 1.1763539760067943e-06\n",
      "468 1.169427150671254e-06\n",
      "469 1.166266656582593e-06\n",
      "470 1.160779675046797e-06\n",
      "471 1.1696862429744215e-06\n",
      "472 1.1696014325934811e-06\n",
      "473 1.167726622952614e-06\n",
      "474 1.1745948995667277e-06\n",
      "475 1.1648202189462609e-06\n",
      "476 1.158800614575739e-06\n",
      "477 1.150637103819463e-06\n",
      "478 1.148082674262696e-06\n",
      "479 1.15561158509081e-06\n",
      "480 1.149562194768805e-06\n",
      "481 1.1394332659619977e-06\n",
      "482 1.1496061915750033e-06\n",
      "483 1.1562378858798184e-06\n",
      "484 1.1544618701009313e-06\n",
      "485 1.1512197488627862e-06\n",
      "486 1.1532684993653675e-06\n",
      "487 1.1540884088390158e-06\n",
      "488 1.1530178198881913e-06\n",
      "489 1.1563506632228382e-06\n",
      "490 1.1590582289500162e-06\n",
      "491 1.1583015293581411e-06\n",
      "492 1.1528032928254106e-06\n",
      "493 1.1514993047967437e-06\n",
      "494 1.1462835800557514e-06\n",
      "495 1.143808958659065e-06\n",
      "496 1.1375911981303943e-06\n",
      "497 1.1352208275638986e-06\n",
      "498 1.1352624369465047e-06\n",
      "499 1.133960381594079e-06\n"
     ]
    }
   ],
   "source": [
    "def forward(x_in, y_out=None):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    return loss, y_pred, h_relu, h\n",
    "\n",
    "def backward(y_pred, h_relu, h):\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    return grad_w1, grad_w2\n",
    "\n",
    "for t in range(500):\n",
    "    loss, y_pred, h_relu, h = forward(x, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    grad_w1, grad_w2 = backward(y_pred, h_relu, h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## autograd\n",
    "1. set `requires_grad=True` to autograd\n",
    "2. set `torch.no_grad()` to prevent grad update eg. in parameter update\n",
    "3. set zero the gradients after running the backward pass `w1.grad.zero_()`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32561048.0\n",
      "1 31525604.0\n",
      "2 34617940.0\n",
      "3 35333936.0\n",
      "4 30086292.0\n",
      "5 19945222.0\n",
      "6 10826222.0\n",
      "7 5306579.5\n",
      "8 2736589.5\n",
      "9 1614679.75\n",
      "10 1104386.75\n",
      "11 839129.3125\n",
      "12 677808.3125\n",
      "13 565815.75\n",
      "14 480842.0\n",
      "15 413018.375\n",
      "16 357377.0625\n",
      "17 310928.6875\n",
      "18 271725.34375\n",
      "19 238426.9375\n",
      "20 210112.03125\n",
      "21 185766.515625\n",
      "22 164732.21875\n",
      "23 146483.203125\n",
      "24 130587.6484375\n",
      "25 116675.125\n",
      "26 104462.3203125\n",
      "27 93709.734375\n",
      "28 84218.234375\n",
      "29 75822.984375\n",
      "30 68375.84375\n",
      "31 61767.72265625\n",
      "32 55880.515625\n",
      "33 50621.15625\n",
      "34 45913.484375\n",
      "35 41694.5703125\n",
      "36 37905.8671875\n",
      "37 34497.3515625\n",
      "38 31424.947265625\n",
      "39 28654.037109375\n",
      "40 26151.349609375\n",
      "41 23888.080078125\n",
      "42 21839.908203125\n",
      "43 19982.6953125\n",
      "44 18298.2734375\n",
      "45 16768.30859375\n",
      "46 15378.0322265625\n",
      "47 14113.125\n",
      "48 12962.287109375\n",
      "49 11912.8828125\n",
      "50 10955.646484375\n",
      "51 10081.625\n",
      "52 9283.0908203125\n",
      "53 8552.578125\n",
      "54 7884.755859375\n",
      "55 7273.1787109375\n",
      "56 6712.4970703125\n",
      "57 6198.2685546875\n",
      "58 5726.85009765625\n",
      "59 5294.14697265625\n",
      "60 4896.4404296875\n",
      "61 4530.67529296875\n",
      "62 4194.2841796875\n",
      "63 3884.557861328125\n",
      "64 3599.322998046875\n",
      "65 3336.51806640625\n",
      "66 3094.233642578125\n",
      "67 2870.7109375\n",
      "68 2664.4248046875\n",
      "69 2474.13134765625\n",
      "70 2298.95068359375\n",
      "71 2137.16259765625\n",
      "72 1987.61572265625\n",
      "73 1849.16357421875\n",
      "74 1721.038818359375\n",
      "75 1602.3558349609375\n",
      "76 1492.4329833984375\n",
      "77 1390.603271484375\n",
      "78 1296.25927734375\n",
      "79 1208.71630859375\n",
      "80 1127.475830078125\n",
      "81 1052.018310546875\n",
      "82 981.931640625\n",
      "83 916.8117065429688\n",
      "84 856.2500610351562\n",
      "85 799.9432983398438\n",
      "86 747.542236328125\n",
      "87 698.7935791015625\n",
      "88 653.3890991210938\n",
      "89 611.1066284179688\n",
      "90 571.7215576171875\n",
      "91 535.0134887695312\n",
      "92 500.8018798828125\n",
      "93 468.89593505859375\n",
      "94 439.13311767578125\n",
      "95 411.3558044433594\n",
      "96 385.4316101074219\n",
      "97 361.2160949707031\n",
      "98 338.58905029296875\n",
      "99 317.4570617675781\n",
      "100 297.7088928222656\n",
      "101 279.25811767578125\n",
      "102 262.002197265625\n",
      "103 245.86746215820312\n",
      "104 230.772705078125\n",
      "105 216.6472625732422\n",
      "106 203.43026733398438\n",
      "107 191.05426025390625\n",
      "108 179.46607971191406\n",
      "109 168.6150360107422\n",
      "110 158.44895935058594\n",
      "111 148.92164611816406\n",
      "112 139.99560546875\n",
      "113 131.62478637695312\n",
      "114 123.77831268310547\n",
      "115 116.41500854492188\n",
      "116 109.51133728027344\n",
      "117 103.03515625\n",
      "118 96.95862579345703\n",
      "119 91.25235748291016\n",
      "120 85.89663696289062\n",
      "121 80.86737060546875\n",
      "122 76.14401245117188\n",
      "123 71.70935821533203\n",
      "124 67.53778076171875\n",
      "125 63.61985397338867\n",
      "126 59.93914794921875\n",
      "127 56.476070404052734\n",
      "128 53.22370147705078\n",
      "129 50.16206741333008\n",
      "130 47.28410339355469\n",
      "131 44.57732391357422\n",
      "132 42.03000259399414\n",
      "133 39.63254165649414\n",
      "134 37.37660598754883\n",
      "135 35.25338363647461\n",
      "136 33.2535285949707\n",
      "137 31.372529983520508\n",
      "138 29.600345611572266\n",
      "139 27.931333541870117\n",
      "140 26.359514236450195\n",
      "141 24.879413604736328\n",
      "142 23.484804153442383\n",
      "143 22.169300079345703\n",
      "144 20.930944442749023\n",
      "145 19.762657165527344\n",
      "146 18.661083221435547\n",
      "147 17.62316131591797\n",
      "148 16.644643783569336\n",
      "149 15.722108840942383\n",
      "150 14.852312088012695\n",
      "151 14.031269073486328\n",
      "152 13.257221221923828\n",
      "153 12.527637481689453\n",
      "154 11.839241981506348\n",
      "155 11.189689636230469\n",
      "156 10.577279090881348\n",
      "157 9.999166488647461\n",
      "158 9.453089714050293\n",
      "159 8.937905311584473\n",
      "160 8.45130729675293\n",
      "161 7.9911298751831055\n",
      "162 7.5569257736206055\n",
      "163 7.146893501281738\n",
      "164 6.760406970977783\n",
      "165 6.393991947174072\n",
      "166 6.048830986022949\n",
      "167 5.722506999969482\n",
      "168 5.4136199951171875\n",
      "169 5.122294902801514\n",
      "170 4.847075939178467\n",
      "171 4.586866855621338\n",
      "172 4.341037750244141\n",
      "173 4.108060836791992\n",
      "174 3.8883919715881348\n",
      "175 3.6802988052368164\n",
      "176 3.4834201335906982\n",
      "177 3.297091245651245\n",
      "178 3.1220829486846924\n",
      "179 2.9557290077209473\n",
      "180 2.7983720302581787\n",
      "181 2.649728775024414\n",
      "182 2.5092194080352783\n",
      "183 2.3761720657348633\n",
      "184 2.2500128746032715\n",
      "185 2.130983352661133\n",
      "186 2.0182087421417236\n",
      "187 1.9113882780075073\n",
      "188 1.8105665445327759\n",
      "189 1.7150402069091797\n",
      "190 1.6247072219848633\n",
      "191 1.5390255451202393\n",
      "192 1.4581657648086548\n",
      "193 1.3814406394958496\n",
      "194 1.3088505268096924\n",
      "195 1.2403675317764282\n",
      "196 1.1752610206604004\n",
      "197 1.1136198043823242\n",
      "198 1.0552337169647217\n",
      "199 1.0001177787780762\n",
      "200 0.9479610323905945\n",
      "201 0.8983155488967896\n",
      "202 0.8513228297233582\n",
      "203 0.8070619106292725\n",
      "204 0.7651363611221313\n",
      "205 0.7251445651054382\n",
      "206 0.6873536109924316\n",
      "207 0.6516793370246887\n",
      "208 0.6177303791046143\n",
      "209 0.5856637358665466\n",
      "210 0.5553362369537354\n",
      "211 0.5264098048210144\n",
      "212 0.4990420639514923\n",
      "213 0.4731712341308594\n",
      "214 0.4487316906452179\n",
      "215 0.4254690110683441\n",
      "216 0.403542160987854\n",
      "217 0.3825989365577698\n",
      "218 0.36281388998031616\n",
      "219 0.3441564440727234\n",
      "220 0.3263448476791382\n",
      "221 0.30941659212112427\n",
      "222 0.29349207878112793\n",
      "223 0.27840474247932434\n",
      "224 0.26411139965057373\n",
      "225 0.25054487586021423\n",
      "226 0.23764532804489136\n",
      "227 0.22540971636772156\n",
      "228 0.21390336751937866\n",
      "229 0.2029087394475937\n",
      "230 0.19243736565113068\n",
      "231 0.18259665369987488\n",
      "232 0.1731775850057602\n",
      "233 0.16431403160095215\n",
      "234 0.1559937447309494\n",
      "235 0.14797672629356384\n",
      "236 0.14035193622112274\n",
      "237 0.13320566713809967\n",
      "238 0.12634263932704926\n",
      "239 0.11994941532611847\n",
      "240 0.11384697258472443\n",
      "241 0.10803557187318802\n",
      "242 0.10253418236970901\n",
      "243 0.09734725207090378\n",
      "244 0.09234782308340073\n",
      "245 0.08765962719917297\n",
      "246 0.0831911489367485\n",
      "247 0.0789325088262558\n",
      "248 0.07494352757930756\n",
      "249 0.07109499722719193\n",
      "250 0.06746985763311386\n",
      "251 0.0640668123960495\n",
      "252 0.060792285948991776\n",
      "253 0.05771796405315399\n",
      "254 0.054776035249233246\n",
      "255 0.05201181396842003\n",
      "256 0.04937833920121193\n",
      "257 0.046893566846847534\n",
      "258 0.044491276144981384\n",
      "259 0.042245764285326004\n",
      "260 0.040096938610076904\n",
      "261 0.03807080537080765\n",
      "262 0.03616398945450783\n",
      "263 0.03432096168398857\n",
      "264 0.03259746730327606\n",
      "265 0.030944885686039925\n",
      "266 0.029377754777669907\n",
      "267 0.027886034920811653\n",
      "268 0.026510318741202354\n",
      "269 0.025161126628518105\n",
      "270 0.02388969250023365\n",
      "271 0.022699028253555298\n",
      "272 0.021586887538433075\n",
      "273 0.020498665049672127\n",
      "274 0.019479144364595413\n",
      "275 0.018493589013814926\n",
      "276 0.01756257750093937\n",
      "277 0.016680825501680374\n",
      "278 0.01585458032786846\n",
      "279 0.015067938715219498\n",
      "280 0.014322465285658836\n",
      "281 0.013615231029689312\n",
      "282 0.01294898334890604\n",
      "283 0.012307943776249886\n",
      "284 0.011704495176672935\n",
      "285 0.011130690574645996\n",
      "286 0.010576865635812283\n",
      "287 0.010052342899143696\n",
      "288 0.009559931233525276\n",
      "289 0.009083802811801434\n",
      "290 0.008644522167742252\n",
      "291 0.008224374614655972\n",
      "292 0.007822608575224876\n",
      "293 0.0074511426500976086\n",
      "294 0.007076281122863293\n",
      "295 0.006729557178914547\n",
      "296 0.006409090012311935\n",
      "297 0.006105982232838869\n",
      "298 0.005816498771309853\n",
      "299 0.005529635585844517\n",
      "300 0.005269933026283979\n",
      "301 0.005019812844693661\n",
      "302 0.0047831228002905846\n",
      "303 0.004551885649561882\n",
      "304 0.004334501922130585\n",
      "305 0.004137987736612558\n",
      "306 0.0039412775076925755\n",
      "307 0.003756436752155423\n",
      "308 0.0035824410151690245\n",
      "309 0.003421594388782978\n",
      "310 0.0032655843533575535\n",
      "311 0.003116233041509986\n",
      "312 0.0029776389710605145\n",
      "313 0.002839520340785384\n",
      "314 0.0027153976261615753\n",
      "315 0.0025941410567611456\n",
      "316 0.0024786298163235188\n",
      "317 0.002369104651734233\n",
      "318 0.002268969314172864\n",
      "319 0.0021709399297833443\n",
      "320 0.0020772982388734818\n",
      "321 0.0019891432020813227\n",
      "322 0.0019075953168794513\n",
      "323 0.0018241177313029766\n",
      "324 0.0017481581307947636\n",
      "325 0.0016773662064224482\n",
      "326 0.0016070313286036253\n",
      "327 0.00154190044850111\n",
      "328 0.0014766176464036107\n",
      "329 0.0014162241714075208\n",
      "330 0.0013600719394162297\n",
      "331 0.0013074233429506421\n",
      "332 0.0012569449609145522\n",
      "333 0.0012044047471135855\n",
      "334 0.0011602556332945824\n",
      "335 0.0011150101199746132\n",
      "336 0.0010736469412222505\n",
      "337 0.0010329416254535317\n",
      "338 0.000995731563307345\n",
      "339 0.0009574134019203484\n",
      "340 0.0009215680183842778\n",
      "341 0.0008888228912837803\n",
      "342 0.0008579122950322926\n",
      "343 0.0008263189811259508\n",
      "344 0.0007962404051795602\n",
      "345 0.0007685429882258177\n",
      "346 0.0007422862108796835\n",
      "347 0.0007158078951761127\n",
      "348 0.0006916085258126259\n",
      "349 0.0006679503712803125\n",
      "350 0.0006434379029087722\n",
      "351 0.0006242716917768121\n",
      "352 0.0006028428324498236\n",
      "353 0.0005826407577842474\n",
      "354 0.0005660025053657591\n",
      "355 0.0005459559033624828\n",
      "356 0.0005295145674608648\n",
      "357 0.0005122650181874633\n",
      "358 0.0004956845077686012\n",
      "359 0.00048125506145879626\n",
      "360 0.0004650344781111926\n",
      "361 0.0004509024729486555\n",
      "362 0.0004366388020571321\n",
      "363 0.00042433853377588093\n",
      "364 0.0004121240635868162\n",
      "365 0.00039932309300638735\n",
      "366 0.0003877273411490023\n",
      "367 0.0003773821226786822\n",
      "368 0.0003668914723675698\n",
      "369 0.0003563276259228587\n",
      "370 0.00034606378176249564\n",
      "371 0.00033588949008844793\n",
      "372 0.00032654815004207194\n",
      "373 0.0003180224448442459\n",
      "374 0.0003091165272053331\n",
      "375 0.0003006292099598795\n",
      "376 0.0002918260870501399\n",
      "377 0.00028418455622158945\n",
      "378 0.0002774441381916404\n",
      "379 0.0002698912867344916\n",
      "380 0.00026234943652525544\n",
      "381 0.00025641851243562996\n",
      "382 0.00024929066421464086\n",
      "383 0.0002438231313135475\n",
      "384 0.00023860979126766324\n",
      "385 0.00023242225870490074\n",
      "386 0.00022662623086944222\n",
      "387 0.00022042766795493662\n",
      "388 0.00021565963106695563\n",
      "389 0.00021035275130998343\n",
      "390 0.00020569446496665478\n",
      "391 0.00020117302483413368\n",
      "392 0.00019597492064349353\n",
      "393 0.00019151561718899757\n",
      "394 0.00018621026538312435\n",
      "395 0.0001824101054808125\n",
      "396 0.00017861707601696253\n",
      "397 0.0001744791225064546\n",
      "398 0.00017045500862877816\n",
      "399 0.00016653940838295966\n",
      "400 0.00016267098544631153\n",
      "401 0.00015904275642242283\n",
      "402 0.000155258720042184\n",
      "403 0.00015180589980445802\n",
      "404 0.0001488720008637756\n",
      "405 0.0001462193758925423\n",
      "406 0.0001429303956683725\n",
      "407 0.00013975655019748956\n",
      "408 0.00013726080942433327\n",
      "409 0.00013445470540318638\n",
      "410 0.00013153807958588004\n",
      "411 0.00012899046123493463\n",
      "412 0.00012677469931077212\n",
      "413 0.00012425376917235553\n",
      "414 0.00012148650421295315\n",
      "415 0.00011940608965232968\n",
      "416 0.00011747262033168226\n",
      "417 0.00011482909030746669\n",
      "418 0.0001129844895331189\n",
      "419 0.00011107843602076173\n",
      "420 0.00010908758122241125\n",
      "421 0.0001074600950232707\n",
      "422 0.00010501289216335863\n",
      "423 0.00010295001266058534\n",
      "424 0.00010109644790645689\n",
      "425 9.884292376227677e-05\n",
      "426 9.744866838445887e-05\n",
      "427 9.530267561785877e-05\n",
      "428 9.347637387691066e-05\n",
      "429 9.132872946793213e-05\n",
      "430 9.013539965962991e-05\n",
      "431 8.874286140780896e-05\n",
      "432 8.73570897965692e-05\n",
      "433 8.571402577217668e-05\n",
      "434 8.424199040746316e-05\n",
      "435 8.309775876114145e-05\n",
      "436 8.174807589966804e-05\n",
      "437 8.026317664189264e-05\n",
      "438 7.906699465820566e-05\n",
      "439 7.783064211253077e-05\n",
      "440 7.649465260328725e-05\n",
      "441 7.545306289102882e-05\n",
      "442 7.416587322950363e-05\n",
      "443 7.328425272135064e-05\n",
      "444 7.223139255074784e-05\n",
      "445 7.101571100065485e-05\n",
      "446 7.016944437054917e-05\n",
      "447 6.876338011352345e-05\n",
      "448 6.810029299231246e-05\n",
      "449 6.671280425507575e-05\n",
      "450 6.580289482371882e-05\n",
      "451 6.497988215414807e-05\n",
      "452 6.39923891867511e-05\n",
      "453 6.31315924692899e-05\n",
      "454 6.246585689950734e-05\n",
      "455 6.155115988804027e-05\n",
      "456 6.068824222893454e-05\n",
      "457 5.9974860050715506e-05\n",
      "458 5.9211124607827514e-05\n",
      "459 5.840543963131495e-05\n",
      "460 5.796485493192449e-05\n",
      "461 5.722175046685152e-05\n",
      "462 5.6640787079231814e-05\n",
      "463 5.570444045588374e-05\n",
      "464 5.475735815707594e-05\n",
      "465 5.399521978688426e-05\n",
      "466 5.311761924531311e-05\n",
      "467 5.2289862651377916e-05\n",
      "468 5.153013989911415e-05\n",
      "469 5.09252495248802e-05\n",
      "470 5.0230057240696624e-05\n",
      "471 4.96256398037076e-05\n",
      "472 4.895165329799056e-05\n",
      "473 4.833194543607533e-05\n",
      "474 4.761711534229107e-05\n",
      "475 4.724161044578068e-05\n",
      "476 4.661327329813503e-05\n",
      "477 4.606347283697687e-05\n",
      "478 4.558185173664242e-05\n",
      "479 4.4994696509093046e-05\n",
      "480 4.425679435371421e-05\n",
      "481 4.3797186663141474e-05\n",
      "482 4.3041189201176167e-05\n",
      "483 4.26576953032054e-05\n",
      "484 4.207241363474168e-05\n",
      "485 4.1499130020383745e-05\n",
      "486 4.1158724343404174e-05\n",
      "487 4.063886808580719e-05\n",
      "488 4.03025042032823e-05\n",
      "489 3.9857899537310004e-05\n",
      "490 3.9526046748505905e-05\n",
      "491 3.9077447581803426e-05\n",
      "492 3.839107739622705e-05\n",
      "493 3.8164966099429876e-05\n",
      "494 3.764158100239001e-05\n",
      "495 3.726268914761022e-05\n",
      "496 3.6729645216837525e-05\n",
      "497 3.6473022191785276e-05\n",
      "498 3.604033190640621e-05\n",
      "499 3.5785429645329714e-05\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "    # is a Python number giving its value.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T09:53:17.449259400Z",
     "start_time": "2023-06-15T09:53:13.216377900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## define our own backward\n",
    "1. define a class extends `torch.autograd.Function`\n",
    "2. overwrite `forward(ctx, x)` must cache output in ctx for backward\n",
    "3. overwrite `backward(ctx, grad_output)` get x from `ctx.saved_tensors`\n",
    "4. use `apply` in forward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27072992.0\n",
      "1 22983770.0\n",
      "2 24267158.0\n",
      "3 27426404.0\n",
      "4 29172010.0\n",
      "5 26555398.0\n",
      "6 19761596.0\n",
      "7 12093381.0\n",
      "8 6518920.5\n",
      "9 3433390.0\n",
      "10 1940608.5\n",
      "11 1238647.0\n",
      "12 891377.5625\n",
      "13 699607.4375\n",
      "14 578775.25\n",
      "15 493089.5\n",
      "16 427031.25\n",
      "17 373475.125\n",
      "18 328727.0\n",
      "19 290725.90625\n",
      "20 258092.359375\n",
      "21 229853.984375\n",
      "22 205287.875\n",
      "23 183812.28125\n",
      "24 164977.640625\n",
      "25 148386.5625\n",
      "26 133740.640625\n",
      "27 120780.703125\n",
      "28 109268.328125\n",
      "29 99035.3359375\n",
      "30 89902.03125\n",
      "31 81733.96875\n",
      "32 74412.078125\n",
      "33 67832.625\n",
      "34 61912.0390625\n",
      "35 56574.34765625\n",
      "36 51755.1796875\n",
      "37 47399.67578125\n",
      "38 43452.2421875\n",
      "39 39870.359375\n",
      "40 36617.29296875\n",
      "41 33659.37890625\n",
      "42 30968.599609375\n",
      "43 28513.65234375\n",
      "44 26273.7265625\n",
      "45 24227.650390625\n",
      "46 22357.7578125\n",
      "47 20646.72265625\n",
      "48 19079.12890625\n",
      "49 17641.958984375\n",
      "50 16324.041015625\n",
      "51 15113.80859375\n",
      "52 14001.2822265625\n",
      "53 12978.232421875\n",
      "54 12036.0595703125\n",
      "55 11168.1103515625\n",
      "56 10367.81640625\n",
      "57 9629.708984375\n",
      "58 8948.5966796875\n",
      "59 8319.46875\n",
      "60 7737.97900390625\n",
      "61 7200.3232421875\n",
      "62 6703.19384765625\n",
      "63 6242.80859375\n",
      "64 5816.521484375\n",
      "65 5420.89599609375\n",
      "66 5054.142578125\n",
      "67 4714.13037109375\n",
      "68 4398.728515625\n",
      "69 4105.986328125\n",
      "70 3834.050537109375\n",
      "71 3581.359130859375\n",
      "72 3346.489013671875\n",
      "73 3128.13330078125\n",
      "74 2925.0654296875\n",
      "75 2735.9833984375\n",
      "76 2559.96826171875\n",
      "77 2396.035400390625\n",
      "78 2243.428955078125\n",
      "79 2101.108642578125\n",
      "80 1968.419677734375\n",
      "81 1844.6234130859375\n",
      "82 1729.154541015625\n",
      "83 1621.408203125\n",
      "84 1520.727783203125\n",
      "85 1426.70654296875\n",
      "86 1338.897216796875\n",
      "87 1256.8114013671875\n",
      "88 1180.2939453125\n",
      "89 1108.72412109375\n",
      "90 1041.75634765625\n",
      "91 979.0941162109375\n",
      "92 920.4302978515625\n",
      "93 865.4846801757812\n",
      "94 814.0419311523438\n",
      "95 765.8313598632812\n",
      "96 720.6478881835938\n",
      "97 678.2755737304688\n",
      "98 638.5360107421875\n",
      "99 601.2609252929688\n",
      "100 566.2991943359375\n",
      "101 533.4793090820312\n",
      "102 502.6811828613281\n",
      "103 473.75579833984375\n",
      "104 446.5846862792969\n",
      "105 421.0492858886719\n",
      "106 397.04949951171875\n",
      "107 374.496826171875\n",
      "108 353.29852294921875\n",
      "109 333.3729553222656\n",
      "110 314.622802734375\n",
      "111 296.9919128417969\n",
      "112 280.40521240234375\n",
      "113 264.7934265136719\n",
      "114 250.0948486328125\n",
      "115 236.25059509277344\n",
      "116 223.22109985351562\n",
      "117 210.9497833251953\n",
      "118 199.3733367919922\n",
      "119 188.4705810546875\n",
      "120 178.1991424560547\n",
      "121 168.50103759765625\n",
      "122 159.35830688476562\n",
      "123 150.73829650878906\n",
      "124 142.60617065429688\n",
      "125 134.9351348876953\n",
      "126 127.68940734863281\n",
      "127 120.85311126708984\n",
      "128 114.39901733398438\n",
      "129 108.30106353759766\n",
      "130 102.54548645019531\n",
      "131 97.11185455322266\n",
      "132 91.97526550292969\n",
      "133 87.12084197998047\n",
      "134 82.53414154052734\n",
      "135 78.20022583007812\n",
      "136 74.1007080078125\n",
      "137 70.22679901123047\n",
      "138 66.5645751953125\n",
      "139 63.10008239746094\n",
      "140 59.82040786743164\n",
      "141 56.71889877319336\n",
      "142 53.78376388549805\n",
      "143 51.00684356689453\n",
      "144 48.37971496582031\n",
      "145 45.89206314086914\n",
      "146 43.53617477416992\n",
      "147 41.30555725097656\n",
      "148 39.19318771362305\n",
      "149 37.195125579833984\n",
      "150 35.299102783203125\n",
      "151 33.505001068115234\n",
      "152 31.8031005859375\n",
      "153 30.192367553710938\n",
      "154 28.666606903076172\n",
      "155 27.21890640258789\n",
      "156 25.846567153930664\n",
      "157 24.544387817382812\n",
      "158 23.3118953704834\n",
      "159 22.14361572265625\n",
      "160 21.03399085998535\n",
      "161 19.981203079223633\n",
      "162 18.98427963256836\n",
      "163 18.037141799926758\n",
      "164 17.14032554626465\n",
      "165 16.28778839111328\n",
      "166 15.479175567626953\n",
      "167 14.713114738464355\n",
      "168 13.985004425048828\n",
      "169 13.293330192565918\n",
      "170 12.636981964111328\n",
      "171 12.014217376708984\n",
      "172 11.422581672668457\n",
      "173 10.861370086669922\n",
      "174 10.328057289123535\n",
      "175 9.821407318115234\n",
      "176 9.33977222442627\n",
      "177 8.883759498596191\n",
      "178 8.449295043945312\n",
      "179 8.037259101867676\n",
      "180 7.645124435424805\n",
      "181 7.273101329803467\n",
      "182 6.920068740844727\n",
      "183 6.583841323852539\n",
      "184 6.265310287475586\n",
      "185 5.960836887359619\n",
      "186 5.672135353088379\n",
      "187 5.397760391235352\n",
      "188 5.137031555175781\n",
      "189 4.888851165771484\n",
      "190 4.653184413909912\n",
      "191 4.429049015045166\n",
      "192 4.216132164001465\n",
      "193 4.013239860534668\n",
      "194 3.821063756942749\n",
      "195 3.6373116970062256\n",
      "196 3.46246337890625\n",
      "197 3.2971343994140625\n",
      "198 3.1390576362609863\n",
      "199 2.9893484115600586\n",
      "200 2.8464341163635254\n",
      "201 2.7106170654296875\n",
      "202 2.5812554359436035\n",
      "203 2.458285093307495\n",
      "204 2.3410398960113525\n",
      "205 2.2297842502593994\n",
      "206 2.1241230964660645\n",
      "207 2.023240089416504\n",
      "208 1.927293062210083\n",
      "209 1.8360509872436523\n",
      "210 1.7489351034164429\n",
      "211 1.6660404205322266\n",
      "212 1.5872902870178223\n",
      "213 1.512080192565918\n",
      "214 1.4407382011413574\n",
      "215 1.3726917505264282\n",
      "216 1.3078951835632324\n",
      "217 1.2461963891983032\n",
      "218 1.187557339668274\n",
      "219 1.1316198110580444\n",
      "220 1.0784962177276611\n",
      "221 1.027718424797058\n",
      "222 0.9793882966041565\n",
      "223 0.9332674741744995\n",
      "224 0.8893454074859619\n",
      "225 0.8477096557617188\n",
      "226 0.8081321716308594\n",
      "227 0.7701736688613892\n",
      "228 0.7342062592506409\n",
      "229 0.6998465061187744\n",
      "230 0.6670907735824585\n",
      "231 0.6359128355979919\n",
      "232 0.6061755418777466\n",
      "233 0.5778750777244568\n",
      "234 0.550817608833313\n",
      "235 0.5251290202140808\n",
      "236 0.5008180141448975\n",
      "237 0.4775789678096771\n",
      "238 0.45538023114204407\n",
      "239 0.4341198205947876\n",
      "240 0.41382479667663574\n",
      "241 0.3946508765220642\n",
      "242 0.37633273005485535\n",
      "243 0.35895800590515137\n",
      "244 0.34229493141174316\n",
      "245 0.32631319761276245\n",
      "246 0.3111911714076996\n",
      "247 0.2967803478240967\n",
      "248 0.2830590009689331\n",
      "249 0.27003005146980286\n",
      "250 0.25746551156044006\n",
      "251 0.24548673629760742\n",
      "252 0.23419757187366486\n",
      "253 0.2233300358057022\n",
      "254 0.21303141117095947\n",
      "255 0.20316267013549805\n",
      "256 0.19382816553115845\n",
      "257 0.18487581610679626\n",
      "258 0.1763344556093216\n",
      "259 0.16826413571834564\n",
      "260 0.16048747301101685\n",
      "261 0.1530771553516388\n",
      "262 0.14606286585330963\n",
      "263 0.1393141895532608\n",
      "264 0.13293829560279846\n",
      "265 0.12683191895484924\n",
      "266 0.12093695253133774\n",
      "267 0.1154051348567009\n",
      "268 0.11016342788934708\n",
      "269 0.10503227263689041\n",
      "270 0.1002732440829277\n",
      "271 0.09562699496746063\n",
      "272 0.09122152626514435\n",
      "273 0.08703009784221649\n",
      "274 0.08306993544101715\n",
      "275 0.07925255596637726\n",
      "276 0.0756198987364769\n",
      "277 0.07217264175415039\n",
      "278 0.06889964640140533\n",
      "279 0.06575167924165726\n",
      "280 0.06271247565746307\n",
      "281 0.05985373631119728\n",
      "282 0.05712645873427391\n",
      "283 0.054504163563251495\n",
      "284 0.05201195552945137\n",
      "285 0.049624063074588776\n",
      "286 0.047361038625240326\n",
      "287 0.04523606225848198\n",
      "288 0.04316668584942818\n",
      "289 0.04122154042124748\n",
      "290 0.03934440761804581\n",
      "291 0.03753150627017021\n",
      "292 0.035833947360515594\n",
      "293 0.03419366106390953\n",
      "294 0.03261544182896614\n",
      "295 0.031145403161644936\n",
      "296 0.0297608133405447\n",
      "297 0.028417393565177917\n",
      "298 0.027123697102069855\n",
      "299 0.025913504883646965\n",
      "300 0.024745218455791473\n",
      "301 0.02362007461488247\n",
      "302 0.022536691278219223\n",
      "303 0.021522412076592445\n",
      "304 0.020549297332763672\n",
      "305 0.019614920020103455\n",
      "306 0.0187266506254673\n",
      "307 0.01789867877960205\n",
      "308 0.017106203362345695\n",
      "309 0.016329945996403694\n",
      "310 0.015605888329446316\n",
      "311 0.014905061572790146\n",
      "312 0.014241475611925125\n",
      "313 0.01361086592078209\n",
      "314 0.013000953011214733\n",
      "315 0.012423811480402946\n",
      "316 0.011869880370795727\n",
      "317 0.011341914534568787\n",
      "318 0.010832040570676327\n",
      "319 0.010348670184612274\n",
      "320 0.009906313382089138\n",
      "321 0.009470044635236263\n",
      "322 0.009050965309143066\n",
      "323 0.008643083274364471\n",
      "324 0.008276380598545074\n",
      "325 0.007916413247585297\n",
      "326 0.0075738211162388325\n",
      "327 0.007244176231324673\n",
      "328 0.0069343820214271545\n",
      "329 0.006626686546951532\n",
      "330 0.006341036409139633\n",
      "331 0.006077614147216082\n",
      "332 0.005812281277030706\n",
      "333 0.00555899553000927\n",
      "334 0.005323072429746389\n",
      "335 0.005099932663142681\n",
      "336 0.004879200365394354\n",
      "337 0.0046716779470443726\n",
      "338 0.0044716824777424335\n",
      "339 0.004284207243472338\n",
      "340 0.0041052475571632385\n",
      "341 0.003934711217880249\n",
      "342 0.0037678945809602737\n",
      "343 0.00360864051617682\n",
      "344 0.003457598388195038\n",
      "345 0.003320581978186965\n",
      "346 0.003185814246535301\n",
      "347 0.0030581720639020205\n",
      "348 0.002936484757810831\n",
      "349 0.002816641004756093\n",
      "350 0.0027027959004044533\n",
      "351 0.002597639337182045\n",
      "352 0.0024908732157200575\n",
      "353 0.0023983700666576624\n",
      "354 0.0023022270761430264\n",
      "355 0.002213123021647334\n",
      "356 0.002127870451658964\n",
      "357 0.0020472523756325245\n",
      "358 0.0019653341732919216\n",
      "359 0.0018912529340013862\n",
      "360 0.0018197604222223163\n",
      "361 0.0017543141730129719\n",
      "362 0.0016866186633706093\n",
      "363 0.0016228415770456195\n",
      "364 0.001560456003062427\n",
      "365 0.0015062825987115502\n",
      "366 0.0014512611087411642\n",
      "367 0.0013996063498780131\n",
      "368 0.001349133555777371\n",
      "369 0.0013007746310904622\n",
      "370 0.0012533736880868673\n",
      "371 0.0012087675277143717\n",
      "372 0.0011676617432385683\n",
      "373 0.0011291843838989735\n",
      "374 0.001088864984922111\n",
      "375 0.001048066420480609\n",
      "376 0.0010133490432053804\n",
      "377 0.0009802491404116154\n",
      "378 0.0009477832354605198\n",
      "379 0.0009134778520092368\n",
      "380 0.0008863999391905963\n",
      "381 0.0008548918995074928\n",
      "382 0.0008305583614856005\n",
      "383 0.0008047592709772289\n",
      "384 0.0007782498141750693\n",
      "385 0.0007535651093348861\n",
      "386 0.0007301116711460054\n",
      "387 0.0007069213897921145\n",
      "388 0.0006848935154266655\n",
      "389 0.0006647901609539986\n",
      "390 0.0006449669599533081\n",
      "391 0.0006263883551582694\n",
      "392 0.0006077707512304187\n",
      "393 0.0005905717262066901\n",
      "394 0.0005729349213652313\n",
      "395 0.0005565633764490485\n",
      "396 0.0005407460848800838\n",
      "397 0.0005247447406873107\n",
      "398 0.0005105748423375189\n",
      "399 0.0004948145942762494\n",
      "400 0.000481326220324263\n",
      "401 0.000469150603748858\n",
      "402 0.00045599573059007525\n",
      "403 0.0004430593689903617\n",
      "404 0.00043109641410410404\n",
      "405 0.0004204766300972551\n",
      "406 0.00040930553223006427\n",
      "407 0.00039814927731640637\n",
      "408 0.0003876186383422464\n",
      "409 0.0003771180345211178\n",
      "410 0.0003677520726341754\n",
      "411 0.00035757216392084956\n",
      "412 0.00034869794035330415\n",
      "413 0.00034007790964096785\n",
      "414 0.00033107976196333766\n",
      "415 0.0003229757130611688\n",
      "416 0.0003146208473481238\n",
      "417 0.0003071374958381057\n",
      "418 0.0002993008238263428\n",
      "419 0.0002912694471888244\n",
      "420 0.0002850074670277536\n",
      "421 0.0002783910895232111\n",
      "422 0.0002720394404605031\n",
      "423 0.00026539768441580236\n",
      "424 0.0002596392878331244\n",
      "425 0.0002541412250138819\n",
      "426 0.0002471502812113613\n",
      "427 0.00024133501574397087\n",
      "428 0.0002356792101636529\n",
      "429 0.00022998372151050717\n",
      "430 0.00022498794714920223\n",
      "431 0.00021980279416311532\n",
      "432 0.00021524375188164413\n",
      "433 0.00021116691641509533\n",
      "434 0.00020666292402893305\n",
      "435 0.00020174169912934303\n",
      "436 0.00019723697914741933\n",
      "437 0.00019262457499280572\n",
      "438 0.00018923412426374853\n",
      "439 0.000185235810931772\n",
      "440 0.00018125296628568321\n",
      "441 0.000178165573743172\n",
      "442 0.00017429966828785837\n",
      "443 0.00017090426990762353\n",
      "444 0.00016711490752641112\n",
      "445 0.00016374448023270816\n",
      "446 0.00016038825560826808\n",
      "447 0.00015789701137691736\n",
      "448 0.00015539089508820325\n",
      "449 0.0001521135272923857\n",
      "450 0.0001488183334004134\n",
      "451 0.00014629414363298565\n",
      "452 0.0001435097656212747\n",
      "453 0.00014119167462922633\n",
      "454 0.00013809018128085881\n",
      "455 0.0001355191634502262\n",
      "456 0.0001332651445409283\n",
      "457 0.00013017762103118002\n",
      "458 0.00012800194963347167\n",
      "459 0.00012533423432614654\n",
      "460 0.0001234296360053122\n",
      "461 0.00012107504880987108\n",
      "462 0.00011806164548033848\n",
      "463 0.00011655122216325253\n",
      "464 0.00011514029029058293\n",
      "465 0.00011319993063807487\n",
      "466 0.00011166158219566569\n",
      "467 0.00010932126315310597\n",
      "468 0.00010731392831075937\n",
      "469 0.0001053939267876558\n",
      "470 0.00010402112093288451\n",
      "471 0.00010204155842075124\n",
      "472 0.00010045308590633795\n",
      "473 9.881726873572916e-05\n",
      "474 9.743885311763734e-05\n",
      "475 9.583139035385102e-05\n",
      "476 9.439972200198099e-05\n",
      "477 9.27713917917572e-05\n",
      "478 9.10101443878375e-05\n",
      "479 8.958359103417024e-05\n",
      "480 8.825321856420487e-05\n",
      "481 8.717434684513137e-05\n",
      "482 8.605832408647984e-05\n",
      "483 8.4974046330899e-05\n",
      "484 8.341242937603965e-05\n",
      "485 8.211028762161732e-05\n",
      "486 8.11145655461587e-05\n",
      "487 7.997898501344025e-05\n",
      "488 7.846540393074974e-05\n",
      "489 7.736236875643954e-05\n",
      "490 7.65949152992107e-05\n",
      "491 7.562437531305477e-05\n",
      "492 7.458069012500346e-05\n",
      "493 7.330699736485258e-05\n",
      "494 7.245685264933854e-05\n",
      "495 7.161555549828336e-05\n",
      "496 7.007078238530084e-05\n",
      "497 6.941437459317967e-05\n",
      "498 6.839045090600848e-05\n",
      "499 6.771918560843915e-05\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_custom_function.py\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a context object and a Tensor containing the\n",
    "        input; we must return a Tensor containing the output, and we can use the\n",
    "        context object to cache objects for use in the backward pass.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive the context object and a Tensor containing\n",
    "        the gradient of the loss with respect to the output produced during the\n",
    "        forward pass. We can retrieve cached data from the context object, and must\n",
    "        compute and return the gradient of the loss with respect to the input to the\n",
    "        forward function.\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; we call our\n",
    "    # custom ReLU implementation using the MyReLU.apply function\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Update weights using gradient descent\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T10:50:01.065920600Z",
     "start_time": "2023-06-15T10:49:58.377783600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## use nn package to simplify forward\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 665.5538940429688 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "1 617.1970825195312 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "2 575.0623779296875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "3 537.6188354492188 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "4 504.3430480957031 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "5 474.08880615234375 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "6 446.5764465332031 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "7 421.4475402832031 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "8 398.20391845703125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "9 376.4761962890625 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "10 356.02093505859375 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "11 336.80340576171875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "12 318.67193603515625 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "13 301.52606201171875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "14 285.2677917480469 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "15 269.74267578125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "16 254.95773315429688 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "17 240.82301330566406 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "18 227.33416748046875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "19 214.50067138671875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "20 202.27984619140625 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "21 190.6268310546875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "22 179.54388427734375 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "23 169.01718139648438 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "24 159.0539093017578 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "25 149.60018920898438 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "26 140.68409729003906 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "27 132.25985717773438 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "28 124.29447937011719 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "29 116.74836730957031 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "30 109.63582611083984 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "31 102.92728424072266 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "32 96.61505889892578 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "33 90.69580078125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "34 85.13199615478516 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "35 79.91123962402344 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "36 75.01768493652344 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "37 70.4345932006836 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "38 66.13916015625 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "39 62.12335205078125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "40 58.3571662902832 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "41 54.8309326171875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "42 51.535552978515625 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "43 48.45662307739258 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "44 45.572471618652344 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "45 42.873077392578125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "46 40.345558166503906 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "47 37.98094940185547 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "48 35.7586669921875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "49 33.67442321777344 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "50 31.725582122802734 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "51 29.896259307861328 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "52 28.1850528717041 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "53 26.57827377319336 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "54 25.07103729248047 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "55 23.65479278564453 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "56 22.325876235961914 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "57 21.080232620239258 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "58 19.905052185058594 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "59 18.800556182861328 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "60 17.764650344848633 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "61 16.794824600219727 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "62 15.8822021484375 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "63 15.025308609008789 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "64 14.218537330627441 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "65 13.458438873291016 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "66 12.741209983825684 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "67 12.065877914428711 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "68 11.430578231811523 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "69 10.83156967163086 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "70 10.266663551330566 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "71 9.734262466430664 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "72 9.231569290161133 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "73 8.756680488586426 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "74 8.30915355682373 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "75 7.886351585388184 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "76 7.486687183380127 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "77 7.109195709228516 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "78 6.752941131591797 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "79 6.415677070617676 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "80 6.095158576965332 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "81 5.7920403480529785 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "82 5.504861354827881 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "83 5.232667446136475 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "84 4.974697113037109 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "85 4.730264186859131 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "86 4.498891830444336 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "87 4.279301643371582 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "88 4.071148872375488 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "89 3.8739562034606934 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "90 3.686795473098755 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "91 3.509089231491089 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "92 3.340365171432495 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "93 3.180105686187744 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "94 3.027827024459839 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "95 2.883356809616089 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "96 2.7464287281036377 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "97 2.616832733154297 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "98 2.4935219287872314 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "99 2.3762285709381104 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "100 2.264666795730591 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "101 2.158655881881714 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "102 2.0578341484069824 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "103 1.9618830680847168 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "104 1.8706891536712646 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "105 1.783898949623108 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "106 1.7012377977371216 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "107 1.6226028203964233 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "108 1.5476726293563843 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "109 1.4763031005859375 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "110 1.4083266258239746 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "111 1.3436369895935059 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "112 1.2820383310317993 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "113 1.2233054637908936 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "114 1.1673909425735474 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "115 1.1141220331192017 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "116 1.0633912086486816 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "117 1.014988899230957 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "118 0.9689295291900635 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "119 0.9250313639640808 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "120 0.8831902742385864 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "121 0.8433080315589905 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "122 0.8052623271942139 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "123 0.7689743041992188 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "124 0.734363317489624 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "125 0.7013381123542786 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "126 0.6698553562164307 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "127 0.6398509740829468 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "128 0.6112454533576965 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "129 0.5839346051216125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "130 0.5578669309616089 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "131 0.5330197811126709 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "132 0.5093254446983337 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "133 0.48668983578681946 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "134 0.46509087085723877 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "135 0.4444836378097534 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "136 0.42485126852989197 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "137 0.40613996982574463 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "138 0.38832032680511475 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "139 0.3713223338127136 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "140 0.3550938069820404 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "141 0.33960455656051636 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "142 0.3248240351676941 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "143 0.3106970489025116 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "144 0.2972028851509094 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "145 0.28431326150894165 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "146 0.27200400829315186 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "147 0.2602385878562927 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "148 0.2490045726299286 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "149 0.23827436566352844 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "150 0.22802169620990753 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "151 0.21822625398635864 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "152 0.20886503159999847 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "153 0.19992373883724213 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "154 0.19138240814208984 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "155 0.1832200139760971 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "156 0.17542077600955963 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "157 0.16796551644802094 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "158 0.1608397662639618 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "159 0.1540219783782959 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "160 0.14751003682613373 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "161 0.1412777304649353 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "162 0.13532443344593048 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "163 0.1296236664056778 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "164 0.12417557835578918 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "165 0.1189563050866127 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "166 0.11396738886833191 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "167 0.10919343680143356 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "168 0.10462221503257751 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "169 0.10024747997522354 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "170 0.09606507420539856 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "171 0.09206137806177139 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "172 0.08822739124298096 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "173 0.08456005901098251 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "174 0.08105190098285675 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "175 0.07769370079040527 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "176 0.07447823137044907 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "177 0.07140181958675385 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "178 0.06845615804195404 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "179 0.06563461571931839 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "180 0.0629315972328186 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "181 0.060343630611896515 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "182 0.05786069482564926 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "183 0.0554855614900589 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "184 0.05321068689227104 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "185 0.051029764115810394 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "186 0.04894288629293442 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "187 0.04694344475865364 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "188 0.04502922669053078 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "189 0.04319680482149124 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "190 0.04143887385725975 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "191 0.0397549644112587 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "192 0.038142964243888855 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "193 0.036597102880477905 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "194 0.035115499049425125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "195 0.03369531407952309 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "196 0.032334744930267334 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "197 0.031030509620904922 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "198 0.029781697317957878 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "199 0.028584016487002373 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "200 0.02743571810424328 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "201 0.026336288079619408 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "202 0.025281069800257683 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "203 0.024268874898552895 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "204 0.02329838275909424 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "205 0.022370196878910065 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "206 0.021477581933140755 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "207 0.02062206156551838 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "208 0.019801383838057518 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "209 0.01901393197476864 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "210 0.018259704113006592 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "211 0.017535824328660965 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "212 0.016842035576701164 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "213 0.01617635227739811 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "214 0.015539264306426048 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "215 0.014927961863577366 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "216 0.014340844936668873 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "217 0.013777214102447033 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "218 0.013237266801297665 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "219 0.01271881815046072 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "220 0.012220965698361397 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "221 0.011743269860744476 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "222 0.011284652166068554 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "223 0.010844399221241474 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "224 0.010421792045235634 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "225 0.010016010142862797 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "226 0.009626560844480991 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "227 0.009252672083675861 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "228 0.008894115686416626 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "229 0.008549773134291172 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "230 0.008219216018915176 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "231 0.007901777513325214 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "232 0.007596996612846851 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "233 0.007304121274501085 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "234 0.0070229279808700085 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "235 0.0067529138177633286 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "236 0.006493307650089264 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "237 0.006244205869734287 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "238 0.006004931405186653 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "239 0.005775016266852617 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "240 0.005554600618779659 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "241 0.005342401564121246 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "242 0.005138495936989784 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "243 0.004942632745951414 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "244 0.004754428751766682 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "245 0.0045735593885183334 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "246 0.00439969077706337 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "247 0.0042326790280640125 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "248 0.004072167444974184 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "249 0.003917963244020939 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "250 0.0037698016967624426 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "251 0.00362745625898242 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "252 0.0034905089996755123 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "253 0.003359101479873061 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "254 0.0032326383516192436 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "255 0.0031111205462366343 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "256 0.0029941967222839594 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "257 0.0028819143772125244 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "258 0.002773873507976532 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "259 0.0026700273156166077 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "260 0.002570136683061719 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "261 0.002474159235134721 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "262 0.002382004866376519 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "263 0.002293333178386092 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "264 0.002208008663728833 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "265 0.0021259572822600603 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "266 0.0020471487659960985 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "267 0.001971229212358594 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "268 0.0018981390167027712 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "269 0.0018278044881299138 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "270 0.0017602188745513558 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "271 0.0016951709985733032 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "272 0.0016326142940670252 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "273 0.0015724058030173182 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "274 0.0015145010547712445 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "275 0.0014588002813979983 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "276 0.0014051760081201792 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "277 0.0013536078622564673 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "278 0.0013040199410170317 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "279 0.0012563379714265466 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "280 0.0012103383196517825 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "281 0.0011661184253171086 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "282 0.0011235406855121255 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "283 0.0010825862409546971 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "284 0.0010431985137984157 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "285 0.0010052522411569953 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "286 0.000968682172242552 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "287 0.0009334709611721337 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "288 0.0008996263495646417 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "289 0.0008670357638038695 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "290 0.0008356383186765015 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "291 0.0008054311037994921 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "292 0.0007763384492136538 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "293 0.0007483545341528952 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "294 0.000721371965482831 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "295 0.0006953875999897718 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "296 0.0006703571416437626 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "297 0.000646267260890454 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "298 0.0006230250000953674 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "299 0.0006007119663991034 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "300 0.0005792017909698188 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "301 0.0005584690952673554 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "302 0.0005384685355238616 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "303 0.0005192424287088215 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "304 0.000500721565913409 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "305 0.00048288991092704237 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "306 0.00046569330152124166 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "307 0.00044913034071214497 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "308 0.0004331918025854975 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "309 0.0004178021918050945 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "310 0.0004029825795441866 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "311 0.0003886888734996319 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "312 0.00037492738920263946 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "313 0.00036166663630865514 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "314 0.00034887302899733186 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "315 0.00033656807499937713 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "316 0.0003247105050832033 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "317 0.00031327627948485315 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "318 0.0003022475866600871 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "319 0.00029160897247493267 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "320 0.00028135834145359695 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "321 0.0002714757574722171 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "322 0.000261969689745456 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "323 0.0002528117620386183 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "324 0.00024394287902396172 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "325 0.00023543121642433107 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "326 0.00022722037101630121 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "327 0.000219291279790923 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "328 0.0002116444957209751 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "329 0.00020427818526513875 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "330 0.0001971674500964582 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "331 0.00019030942348763347 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "332 0.00018369816825725138 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "333 0.0001773121621226892 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "334 0.00017117487732321024 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "335 0.00016523341764695942 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "336 0.00015952344983816147 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "337 0.00015401934797409922 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "338 0.00014869641745463014 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "339 0.00014357005420606583 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "340 0.00013862401829101145 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "341 0.0001338524161837995 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "342 0.0001292559754801914 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "343 0.00012481777230277658 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "344 0.00012053664977429435 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "345 0.00011641488526947796 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "346 0.00011242330947425216 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "347 0.00010858511814149097 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "348 0.00010487675899639726 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "349 0.00010129746078746393 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "350 9.783202403923497e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "351 9.450221841689199e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "352 9.128035162575543e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "353 8.817981142783538e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "354 8.517699461663142e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "355 8.229620289057493e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "356 7.950340659590438e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "357 7.680723501835018e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "358 7.420585461659357e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "359 7.169661694206297e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "360 6.927160575287417e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "361 6.693082104902714e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "362 6.466906779678538e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "363 6.248829595278949e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "364 6.0385405959095806e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "365 5.8354518841952085e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "366 5.6389017117908224e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "367 5.449845048133284e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "368 5.26620278833434e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "369 5.089926708023995e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "370 4.9187590775545686e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "371 4.75436681881547e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "372 4.595985228661448e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "373 4.4423035433283076e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "374 4.293860183679499e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "375 4.150503082200885e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "376 4.0114959119819105e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "377 3.878140705637634e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "378 3.748946255655028e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "379 3.624209784902632e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "380 3.503501648083329e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "381 3.387615652172826e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "382 3.275297422078438e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "383 3.1667099392507225e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "384 3.0614064598921686e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "385 2.960275742225349e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "386 2.8618322176043876e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "387 2.7675092496792786e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "388 2.6760168111650273e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "389 2.5877063308143988e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "390 2.502323332009837e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "391 2.4200737243518233e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "392 2.3404445528285578e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "393 2.263395072077401e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "394 2.1890671632718295e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "395 2.1171161279198714e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "396 2.047583257080987e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "397 1.980390698008705e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "398 1.9154003894072957e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "399 1.852850618888624e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "400 1.7922118786373176e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "401 1.7336087694275193e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "402 1.676894498814363e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "403 1.622099443920888e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "404 1.56898040586384e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "405 1.5177669411059469e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "406 1.4685525457025506e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "407 1.420876469637733e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "408 1.37472634378355e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "409 1.3299049896886572e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "410 1.2868605153926183e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "411 1.245106068381574e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "412 1.2046679330524057e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "413 1.1655578418867663e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "414 1.1279050340817776e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "415 1.0914919585047755e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "416 1.0562705938355066e-05 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "417 1.0220216609013733e-05 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "418 9.890521141642239e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "419 9.570481779519469e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "420 9.26258053368656e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "421 8.964483640738763e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "422 8.672998774272855e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "423 8.395487384404987e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "424 8.125174645101652e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "425 7.86549844633555e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "426 7.611438377352897e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "427 7.36799029255053e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "428 7.130779977160273e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "429 6.903149824211141e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "430 6.6827233240474015e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "431 6.467713319580071e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "432 6.261780981731135e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "433 6.061413841962349e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "434 5.868769221706316e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "435 5.6811013564583845e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "436 5.5016353144310415e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "437 5.325035999703687e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "438 5.153438905836083e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "439 4.990066827303963e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "440 4.83195162814809e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "441 4.6768841457378585e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "442 4.529965735855512e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "443 4.386192358651897e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "444 4.248031928000273e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "445 4.112331225769594e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "446 3.981031113653444e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "447 3.855738214042503e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "448 3.7333168165787356e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "449 3.6145909234619467e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "450 3.501034143482684e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "451 3.390565325389616e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "452 3.283132173237391e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "453 3.1783963549969485e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "454 3.0786950446781702e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "455 2.9814957542839693e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "456 2.8881756861665053e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "457 2.798532477754634e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "458 2.709253294597147e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "459 2.62374442172586e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "460 2.5408528472325997e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "461 2.4615260372229386e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "462 2.3840825633669738e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "463 2.3094951302482514e-06 <class 'torch.Tensor'>\n",
      "rate tensor(3)\n",
      "464 2.237632770629716e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "465 2.1663663574145176e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "466 2.0991681140003493e-06 <class 'torch.Tensor'>\n",
      "rate tensor(3)\n",
      "467 2.0333116026449716e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "468 1.9700889879459282e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "469 1.907708337967051e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "470 1.8486949784346507e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "471 1.7912675502884667e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "472 1.7348942265016376e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "473 1.681601133896038e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "474 1.6288948927467573e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "475 1.5785544746904634e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "476 1.5299769984267186e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "477 1.4823290257481858e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "478 1.4361517060024198e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "479 1.3916071566200117e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "480 1.3475860214384738e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "481 1.3059775483270641e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "482 1.2651299812205252e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "483 1.225811956828693e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "484 1.1884006880791276e-06 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "485 1.1515595588207361e-06 <class 'torch.Tensor'>\n",
      "rate tensor(3)\n",
      "486 1.1160442454638542e-06 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "487 1.08175254354137e-06 <class 'torch.Tensor'>\n",
      "rate tensor(4)\n",
      "488 1.0481755907676416e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "489 1.01586670098186e-06 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "490 9.84324969977024e-07 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "491 9.542122825223487e-07 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "492 9.245638921129284e-07 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "493 8.959464139479678e-07 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "494 8.686130286150728e-07 <class 'torch.Tensor'>\n",
      "rate tensor(3)\n",
      "495 8.417331400778494e-07 <class 'torch.Tensor'>\n",
      "rate tensor(2)\n",
      "496 8.159673825502978e-07 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n",
      "497 7.909364967417787e-07 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "498 7.667101726838155e-07 <class 'torch.Tensor'>\n",
      "rate tensor(0)\n",
      "499 7.430123218910012e-07 <class 'torch.Tensor'>\n",
      "rate tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Code in file nn/two_layer_net_nn.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# After constructing the model we use the .to() method to move it to the\n",
    "# desired device.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "# reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "# than the mean; this is for consistency with the examples above where we\n",
    "# manually compute the loss, but in practice it is more common to use mean\n",
    "# squared error as a loss by setting reduction='elementwise_mean'.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # output\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item(), type(loss))   # convert tensor to number only work for one number\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-15T11:08:28.674395600Z",
     "start_time": "2023-06-15T11:08:24.201434200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
